{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center;\">\n",
    "     <img src=\"images/gdd-logo.png\" alt=\"Drawing\" style=\"width: 50%;\"/>\n",
    "</p>\n",
    "\n",
    "> __Author__: Rodrigo Agundez\n",
    ">\n",
    "__Date__: 2017-01-17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feed forward deep neural networks\n",
    "\n",
    "Requirements for this notebook:\n",
    " - matplotlib\n",
    " - tensorflow (> 1.4) or keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Overview\n",
    "\n",
    "### Goal\n",
    "\n",
    "- Get familiar with the Keras sequential API\n",
    "- Build a DNN and classify images with 10 digit classes\n",
    "- Play around with the API\n",
    "\n",
    "### Specific tasks\n",
    "\n",
    "1. The code works for the easy MNIST dataset `datasets.mnist.load_data()`, run it and make sense of it, ask questions!\n",
    "2. Change the code to work with the fairly complicated CIFAR10 dataset `datasets.cifar10.load_data()`\n",
    "3. Explore with other DNN architectures: number of layers, layer types, activation functions, optimizers, etc.\n",
    "4. Can be interested to look at the keras [functional API](https://keras.io/getting-started/functional-api-guide/) or the tensorflow [dataset API](https://www.tensorflow.org/programmers_guide/datasets). Which advantages can it bring you?\n",
    "\n",
    "\n",
    "### Questions\n",
    "\n",
    "This Notebook contains some questions labeled __Homework__.\n",
    "Ignore them now but do try to answer them later if you want to get more in-depth understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pkg_resources\n",
    "import random\n",
    "\n",
    "from IPython.display import SVG\n",
    "from itertools import groupby\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = 15, 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't pay much attention to the cell below. \n",
    "It checks the TensorFlow version and decides to load keras from tensorflow or keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    pkg_resources.require('tensorflow>1.4.0')\n",
    "except pkg_resources.VersionConflict as e:\n",
    "    print(\"Importing keras\")\n",
    "    import keras\n",
    "else:\n",
    "    print(\"Importing keras from tensorflow\")\n",
    "    from tensorflow.python import keras\n",
    "\n",
    "# some useful defininitions\n",
    "datasets = keras.datasets\n",
    "Dense = keras.layers.Dense\n",
    "models = keras.models\n",
    "kutils = keras.utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like many other libraries, `keras` includes some standard datasets to play around with.\n",
    "The follow a specific [API](https://www.tensorflow.org/programmers_guide/datasets) that make them iteract nicely with TensorFlow. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the MNIST dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you've tried this Notebook and made sense of the code, load the CIFAR10 dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (x_train, y_train), (x_test, y_test) = datasets.cifar10.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"Train:\\tX shape:{x_train.shape}\\tY shape:{y_train.shape}\\tType: {type(y_train)}[{y_train.dtype}]\")\n",
    "print(f\"Test:\\tX shape:{x_test.shape}(x_train, y_train), (x_test, y_test) = datasets.mnist.load_data()\\tY shape:{y_test.shape}\\tType: {type(y_test)}[{y_test.dtype}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Homework: Dataset summary\n",
    ">\n",
    "- How many training examples do we have?\n",
    "- How many color channels does each picture have?\n",
    "- What will the input size to the DNN be?\n",
    "- What will the output size of the DNN be? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show some example images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "[ax.imshow(random.choice(x_train), cmap='gray') for ax in plt.subplots(1, 6)[1]];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our model we'll have to do some preprocessing.\n",
    "\n",
    "We'll process:\n",
    "\n",
    "- each input image into a 1D-array of pixel values\n",
    "- each target label into a categorical vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784\n",
    "output_size = 10\n",
    "\n",
    "\n",
    "x_train_flat = x_train.reshape(len(x_train), input_size).astype('float') / 255\n",
    "x_test_flat = x_test.reshape(len(x_test), input_size).astype('float') / 255\n",
    "\n",
    "y_train_cat = kutils.to_categorical(y_train, output_size)\n",
    "y_test_cat = kutils.to_categorical(y_test, output_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Homework: Preprocessing\n",
    ">\n",
    "- What happens if you don't change the type to float?\n",
    "- What happens if no normalization is done (divide by 255)?\n",
    "- Which other type of normalization could you do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the resulting dimensions and types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Train:\\tX shape:{x_train_flat.shape}\\tY shape:{y_train_cat.shape}\\tType: {type(y_train_cat)}[{y_train_cat.dtype}]\")\n",
    "print(f\"Test:\\tX shape:{x_test_flat.shape}\\tY shape:{y_test_cat.shape}\\tType: {type(y_test_cat)}[{y_test_cat.dtype}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Homework: Model input & output\n",
    "- Are the input and output sizes correct from what you thought?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Model\n",
    "\n",
    "Our model takes pixel values of images and has to classify which digit is on the image.\n",
    "We'll build a model consisting of multiple dense layers - this may not be the optimal way to do image classification but it's good enough to demonstrate the API of `keras`.\n",
    "\n",
    "We will use the Keras [sequential API](https://keras.io/models/sequential/).\n",
    "If you are up for a bit more challenge write the DNN using the [functional API](https://keras.io/getting-started/functional-api-guide/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we define the model type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we directly add the hidden layers to the model using `model.add()`.\n",
    "For each `Dense()` layer, you can specify its name, the number of units, its activation function, etc.\n",
    "\n",
    "But where is the input layer?<br>Keras takes a simple approach and defines it together with the first hidden layer via the parameter `input_dim` or `input_shape`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(name='FullyConnected_0', units=260, input_dim=input_size, activation='relu'))\n",
    "model.add(Dense(name='FullyConnected_1', units=120, activation='relu'))\n",
    "model.add(Dense(name='FullyConnected_2', units=40, activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Homework: Layers\n",
    ">\n",
    "- Which other activations could you use? Check out the [list of activations](https://keras.io/activations/).\n",
    "- Which other type of layers are possible? Run `dir(keras.layers)` and behold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally we add the output layer.\n",
    "The `'softmax'` activation function translates the outputs of the previous layer into probabilities for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(name='FullyConnected_OutputLayer', units=output_size, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models have to be compiled before we train our DNN.\n",
    "We tell `keras` to optimize our neural network using [Adam](http://ruder.io/optimizing-gradient-descent/index.html#adam); the categorical cross-entropy as the loss will make sure that we optimize for good probabilities for all classes; the `metrics` arguments tells which metric to report the score on.\n",
    "\n",
    "The metric(s) set can be used to evaluate over the test dataset, but also, if at trainining time we define `validation_split`, a validation test will be performed over each epoch. This is very helpful to asses the health of our model (overfitting for example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='Adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Homework\n",
    ">\n",
    "- Which other optimizers are available? Check the [list of optimizers](https://keras.io/optimizers/)\n",
    "- How many losses are there avilable? Check the [list of loss functions](https://keras.io/losses/)\n",
    "- What other metrics? [List of metrics](https://keras.io/metrics/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Model overview\n",
    "\n",
    "So we have created our model, nice! Let's now view some key aspects about it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.summary() # very helpful overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"There are a total of {model.count_params()} to calculate. Could you derive the number exactly? What are all of these parameters?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Homework: Models\n",
    ">\n",
    "Check some famous models.\n",
    "- Can you understand and compare the models using the different characteristics?\n",
    "- How does the number of parameters in your model compare to the ones in the table?\n",
    "- What impact does the number of parameters have on accuracy, training time, prediction time, etc.?\n",
    "- What impact does the depth size have on accuracy, training time, prediction time, etc.?\n",
    "\n",
    "![](models_list.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you really want the complete picture, check the configuration. Don't worry if you don't understand what all the parameters mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.get_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Model training\n",
    "\n",
    "Once the model is compiled we can fit our model using the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    x_train_flat, y_train_cat,\n",
    "    batch_size=500,\n",
    "    epochs=10,\n",
    "    validation_split=0.1,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Homework: Training\n",
    ">\n",
    "- Why is it only training on `54000` examples?\n",
    "- What is the difference between `acc` and `val_acc`?\n",
    "- What are `epochs` and `batch_size`? How do they affect accuracy and training time? Play with the numbers\n",
    "- Is the model overfitting? How could you discover possible overfitting problems with this output?\n",
    "- How else can you asses the health of your model?\n",
    "- Can you tell from the output which hyperparameters to change and how (e.g. learning rate)? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how our model does for our completely unseen test set using `model.evaluate()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score, acc = model.evaluate(x_test_flat, y_test_cat, verbose=0)\n",
    "\n",
    "print('Loss on test set:', score)\n",
    "print('Accuracy on test set:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Homework: Validation\n",
    "> \n",
    "- Is this a good accuracy? what other factors need to be assess before answering this question?\n",
    "- How would you go on to make hyperparameter optimization from here?\n",
    "- Which are the hyperparameters?\n",
    "- If you go for hyperparameter optimization, how would you now split the initial dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Evolution of metrics during training\n",
    "\n",
    "Let's see how the metrics evolved over time during training.\n",
    "The returned object from `model.fit` contains the metric history during all epochs (we called it history). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = sorted(history.history.items(), key=lambda x: (x[0].replace('val_', ''), x[0]))\n",
    "for metric, values in groupby(hist, key=lambda x: x[0].replace('val_', '')):\n",
    "    val0, val1 = list(values)\n",
    "    plt.plot(history.epoch, val0[1], history.epoch, val1[1], '--', marker='o')\n",
    "    plt.xlabel('epoch'), plt.ylabel(val0[0]), plt.legend(('Train set', 'Validation set'))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Various information and model diagnosis can be derived from the above plots. Can you point out some key properties?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Analysis of misclassifications\n",
    "\n",
    "- Where are the misclassifications at?\n",
    "- Is there a relation between classes and misclassifications?\n",
    "\n",
    "Note: *You will normally see this type of aggregations and plotting using pandas, I just wanted to have some fun #nopandas*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's collect the predictions on our test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted = model.predict_classes(x_test_flat, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see the error percentage per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "comparison = sorted(zip(y_test, y_predicted, y_predicted == y_test), key=lambda x: x[0])\n",
    "for c, v in groupby(comparison, key=lambda x: x[0]):\n",
    "    v = [x[2] for x in v]\n",
    "    plt.barh(c,  (1 - sum(v) / len(v)) * 100)\n",
    "plt.xlabel('Miss-classification %'), plt.ylabel('True class')\n",
    "plt.yticks(list({x[0] for x in comparison}));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relation between true classes and predicted classes for miss-classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for label in range(10):\n",
    "    miss = filter(lambda x: not x[2] and x[1] == label, comparison)\n",
    "    for (true_class, pred_class), g_data in groupby(miss, key=lambda x: (x[0], x[1])):\n",
    "        plt.barh(true_class, sum(1 for _ in g_data), color='skyblue')\n",
    "    plt.title(f'True class: \"{label}\"')\n",
    "    plt.xlabel('Number of predictions'), plt.ylabel('Predicted class')\n",
    "    plt.yticks(list({x[0] for x in comparison})), plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This type of information over our deep learning model is very important for business/real-life situations.\n",
    "\n",
    "- Where is my model making mistakes?\n",
    "- Is there a classification class I can trust more than others?\n",
    "- Should I definitely not trust certain class predictions? Does it make sense to keep the class in the model then?\n",
    "- This type of info can point out certain corner cases, or special drawbacks not apparent at first \"overview\" look."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Note on learning curves\n",
    "\n",
    "Missing plots here are the learning curve based on the number of examples. They give powerful insights to diagnose your model.\n",
    "\n",
    "They also help you answer, a very important and frequent question in a business scenario:\n",
    "<br>Should I invest in collecting more data? From a data science perspective the answer is yes ofcourse but\n",
    "\n",
    "- Very often obtaining new or more data is very very expensive (â‚¬)\n",
    "- It involves many parts moving together in an organization\n",
    "- Difficult to quantitatively assess the impact the new data will have\n",
    "\n",
    "*Find [here]([learning curves](http://www.ritchieng.com/machinelearning-learning-curve/) good summary on learning curves*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Output\n",
    "\n",
    "There are several ways we can output a model:\n",
    "\n",
    "- Graphical representation for a presentation/demo, for archival purposes, to easily explain others, etc.\n",
    "- Save a model and reuse it later in another application. This is specially important when deploying a DNN models in production.\n",
    "- Transfer learning: save a model and use (parts of) it for different problems.\n",
    "- Several other use cases are applicable where the shipping of the model is essential."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Graphical representation\n",
    "\n",
    "*This functionality was just been fixed on Nov 17, 2017 but up to now is not included in the pip package. I loaded the code into a separte script called utils in this directory. [Here is the related commit](https://github.com/tensorflow/tensorflow/commit/3e53570d3bf518ec2b6cfeed4b5fd57d11370289).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import model_to_dot\n",
    "from utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# get graphical representation of the model\n",
    "SVG(model_to_dot(model, show_shapes=True, rankdir='TB').create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the graphical representation into a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model, show_shapes=True, to_file='code_breaskfast_exercise_1_model.png', rankdir='LR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Write/read model to/from disk\n",
    "\n",
    "There are different ways to save you model into disk. You can save the complete object, just the weights, the configuration, etc. Each option of course includes different aspects of the model.\n",
    "\n",
    "[Take a look at the documented options](https://keras.io/getting-started/faq/#how-can-i-save-a-keras-model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('code_breaskfast_exercise_1_model.h5')\n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What are the differences between each type of saving option?\n",
    "- Which cases are applicable for each?\n",
    "- How is the size of the file related to the saving option? Can you explain these differences?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = keras.models.load_model('code_breaskfast_exercise_1_model.h5')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 Conclusion\n",
    "\n",
    "We've seen how load datasets in `keras`, define a model consisting of multiple learnings, and how to train and validate a model.\n",
    "Check the [examples folder](https://github.com/keras-team/keras/tree/master/examples) in the keras repo if you want to learn more!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:code-breakfast]",
   "language": "python",
   "name": "conda-env-code-breakfast-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
